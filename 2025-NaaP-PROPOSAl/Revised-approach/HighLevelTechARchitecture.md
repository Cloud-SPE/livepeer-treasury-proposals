This technical architecture design outlines the implementation of the **Network-as-a-Product (NaaP) MVP**, transitioning from a localized data setup to a scalable, distributed **Modern Data Lakehouse**. The primary goal is to make NaaP key characteristics measurable and monitorable in real-time.

---

# Technical Architecture: NaaP MVP Metrics & Analytics Infrastructure

## 1. Executive Summary

This architecture provides the foundation for Milestone 1 of the NaaP roadmap: delivering SLA metrics reporting through a distributed analytics pipeline. It decouples event ingestion from heavy analytical processing, ensuring that performance and reliability KPIs—such as **E2E Latency, Jitter, and Failure Rates**—can be computed at scale.

---

## 2. System Architecture Overview

### A. Ingestion Layer (The Source)

* **Push Events**: Go-Livepeer nodes (Gateways and Broadcasters) push real-time events directly to **Apache Kafka**.
* **Pull Scraper**: An asynchronous service scrapes HTTP endpoints from Orchestrator nodes to collect supplemental metadata and publishes it to Kafka to unify the data flow.
* 
**Kafka Connect**: Utilizes an S3 Sink connector to batch stream data from Kafka into **Parquet** files stored on **MinIO**.



### B. Storage & Table Format (The Lake)

* **Object Storage**: **MinIO** serves as the persistent landing zone for raw Parquet data.
* **Table Format**: **Apache Iceberg** is implemented over the Parquet files. This provides ACID transactions, schema evolution, and hidden partitioning, allowing the "lake" to be queried like a traditional relational database without the "Malformed JSON" errors typical of raw file access.
* **Catalog**: **Hive Metastore (HMS)** or **Project Nessie** maintains the metadata for Iceberg tables, tracking snapshots and data file locations for the query engine.

### C. Transformation Layer (The "TODO" / Brains)

* **Distributed Query Engine**: **Trino** serves as the compute muscle. It performs complex, multi-stage joins across massive datasets in S3.


* **Transformation Logic (dbt)**: **dbt (data build tool)** manages the SQL transformation pipeline. It orchestrates the flow from raw event tables to "Silver" cleaned views and finally to "Gold" KPI tables.
* 
**Logic**: dbt matches `gateway_receive_stream_request` events with `gateway_receive_first_processed_segment` using `request_id` to calculate latency.





### D. Serving & Dashboarding Layer (The Hot Layer)

* **Analytics Database**: **ClickHouse** acts as the high-performance serving layer. It stores the finalized aggregates produced by Trino.


* 
**Visualization**: **Metabase** (or a custom Explorer UI) queries ClickHouse to provide Orchestrators and Users with real-time SLA visibility.



---

## 3. Technology Decision Tree

| Technology | Selection Criteria | Decision |
| --- | --- | --- |
| **Compute** | Needs to scale horizontally and join multiple data sources (S3 + Postgres) | **Trino** |
| **Storage Format** | Needs schema safety and high performance on "naked" files | **Apache Iceberg** |
| **Transformation** | Needs version-controlled SQL, modularity, and automated testing | **dbt** |
| **Serving Layer** | Needs sub-second response times for aggregate queries (Avg/Sum) | **ClickHouse** |

---

## 4. Key Performance Indicators (KPIs) Implementation

The following metrics defined in the NaaP MVP spec will be generated by the **dbt + Trino** pipeline:

Core Performance KPIs 

* 
**Output FPS**: Averaged per model from `ai_stream_status` events.


* 
**Prompt-to-First-Frame Latency**: Time delta between request receipt and the first processed frame.


* 
**E2E Stream Latency**: Total time from ingest to egress.


* 
**Jitter Coefficient**: Standard deviation of FPS divided by mean FPS per workflow.


* 
**Startup (Cold) Time**: Latency measured for initial stream initialization.



Reliability & Economic KPIs 

* 
**Inference Failure Rate**: Percentage of `error` type events relative to total requests.


* 
**Swap Rate**: Percentage of requests where the Gateway had to switch Orchestrators.


* 
**CUE (CUDA Utilization Efficiency)**: Calculated as `Achieved FLOPS / Peak FLOPS * 100%`.



---

## 5. Deployment Component Summary (Docker Stack)

To build this, the engineering team requires the following containerized services:

1. **MinIO**: Persistent object storage for the data lake.
2. **Postgres**: Metadata backend for Hive Metastore.
3. **Hive Metastore**: The catalog layer for Iceberg tables.
4. **Trino**: Primary query engine for heavy transformations.
5. 
**ClickHouse**: Fast-access database for dashboards and API consumption.


6. **dbt Runner**: To compile and execute the transformation DAG.

This architecture directly fulfills **Milestone 1.1 (Reference Implementation)** by launching an Explore AI SLA monitoring dashboard with detailed real-time reporting.